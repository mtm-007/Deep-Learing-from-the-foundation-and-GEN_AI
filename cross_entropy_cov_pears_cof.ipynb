{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS Functions\n",
    "\n",
    "\n",
    "> Before we dive in to cross entropy and how its mathematical derivation or the code is formed. Lets take a look at Loss Functions\n",
    "> MSE = Mean Squared Error is a famous loss function used on many applications and models. IT is better on binary classifications, Not a good function for multiple categories (thats where cross entropy is used most)\n",
    "> RMSE is the same as MSE just under a sqrt function (MSE)^1/2 , we will get in depth look later but thats basically what `L2` Loss function is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=torch.rand(3,3)\n",
    "x2 = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE( Mean Squared Error) / L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1839)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE = ((x1-x2)**2).mean()\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RMSE`\n",
    "is just the square root of MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4288)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE = np.sqrt(MSE)\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE ( Mean Absolute Error)/ L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3850)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE = (np.absolute(x1-x2)).mean()\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We will take a look what are the advantages of using MSE or MAE when when to (their Pro and Cons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Decay 0r L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* loss_with_wd = loss + wd *(parameters**2).sum()\n",
    "* parameters.grad + = wd * 2 * parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance\n",
    "- is so close with std Dev\n",
    "- how similiar they are\n",
    "- Here is more explanation from jeremy (fastai) [https://github.com/fastai/course22p2/blob/master/nbs/11_initializing.ipynb]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- var = (t*t).mean() - (m*m)\n",
    "- Cov = ((x - x.mean)*(y-y.mean)).mean()\n",
    "- cov = ( (x*y).mean() - x.mean()*y.mean() )  vs var (X*X).mean() - (m*m) # m is mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cov = ((x - x.mean)*(y-y.mean)).mean()\n",
    "- var = (t*t).mean() - (m*m)\n",
    "- std = (((x - x.mean()).pow(2).mean()).sqrt() * ((y - y.mean()).pow(2).mean()).sqrt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- is just the covarince divide with the produnct of thier stad dev\n",
    "\n",
    "- pear = cov(X,Y)/(stda_dv of x* stda_dev of y)\n",
    "- pear = cov /(x.std() * y.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross- eNtropy\n",
    "- is just the negative log likelyhood with softmax to normalize the likelyhood(probability) to sum up to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

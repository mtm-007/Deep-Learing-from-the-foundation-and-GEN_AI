{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a9e3ec",
   "metadata": {},
   "source": [
    "## Denoising Diffusion Probabilistic Models with miniai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f965c65",
   "metadata": {},
   "source": [
    "\n",
    "Now that we written our own barebones training library, let's make some progress towards exploring diffusion model and building Stable Diffusion from scratch.\n",
    "\n",
    "We'll start with building and training the model described in the seminal 2020 paper Denoising Diffusion Probabilistic Models (DDPM). For more context, while diffusion models were technically invented back in 2015, diffusion models flew under the radar until this 2020 paper since they were complicated and difficult to train. The 2020 paper introducing DDPMs made some crucial assumptions that significantly simplify the model training and generation processes, as we will see here. Later versions of diffusion models all build upon the same framework introduced in this paper.\n",
    "-  Denoising Diffusion Probabilistic Models: https://arxiv.org/abs/2006.11239 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c583962d",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,random,logging\n",
    "import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from fastcore.foundation import L\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb992ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from n_framework import*\n",
    "from activations_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db507b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556e95d",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "- We will load the dataset from HuggingFace Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175de24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c069a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[x] = [TF.resize(TF.to_tensor(o), (32,32)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d9f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 32, 32]), tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "bs = 128\n",
    "tds = dsd.with_transform(transformi)\n",
    "#dls = DataLoaders.from_dd(tds, bs, num_workers=8)    # num_workers aint compatible with mac mps yet\n",
    "dls = DataLoaders.from_dd(tds, bs)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a011e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8561136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMCB(TrainCB):\n",
    "    order = DeviceCB.order+1\n",
    "    def __init__(self, n_steps, beta_min, beta_max):\n",
    "        super().__init__()\n",
    "        self.n_steps,self.βmin,self.βmax = n_steps,beta_min,beta_max\n",
    "        # variance schedule, linearly increased with timestep\n",
    "        self.β = torch.linspace(self.βmin, self.βmax, self.n_steps)\n",
    "        self.α = 1. - self.β \n",
    "        self.ᾱ = torch.cumprod(self.α, dim=0)\n",
    "        self.σ = self.β.sqrt()\n",
    "\n",
    "    def predict(self, learn): learn.preds = learn.model(*learn.batch[0]).sample\n",
    "    \n",
    "    def before_batch(self, learn):\n",
    "        device = learn.batch[0].device\n",
    "        ε = torch.randn(learn.batch[0].shape, device=device)  # noise, x_T\n",
    "        x0 = learn.batch[0] # original images, x_0\n",
    "        self.ᾱ = self.ᾱ.to(device)\n",
    "        n = x0.shape[0]\n",
    "        # select random timesteps\n",
    "        t = torch.randint(0, self.n_steps, (n,), device=device, dtype=torch.long)\n",
    "        ᾱ_t = self.ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n",
    "        xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε #noisify the image\n",
    "        # input to our model is noisy image and timestep, ground truth is the noise \n",
    "        learn.batch = ((xt, t), ε)\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, sz):\n",
    "        device = next(model.parameters()).device\n",
    "        x_t = torch.randn(sz, device=device)\n",
    "        preds = []\n",
    "        for t in reversed(range(self.n_steps)):\n",
    "            t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n",
    "            z = (torch.randn(x_t.shape) if t > 0 else torch.zeros(x_t.shape)).to(device)\n",
    "            ᾱ_t1 = self.ᾱ[t-1]  if t > 0 else torch.tensor(1)\n",
    "            b̄_t = 1 - self.ᾱ[t]\n",
    "            b̄_t1 = 1 - ᾱ_t1\n",
    "            noise_pred = learn.model(x_t, t_batch).sample\n",
    "            x_0_hat = ((x_t - b̄_t.sqrt() * noise_pred)/self.ᾱ[t].sqrt()).clamp(-1,1)\n",
    "            x0_coeff = ᾱ_t1.sqrt()*(1-self.α[t])/b̄_t\n",
    "            xt_coeff = self.α[t].sqrt()*b̄_t1/b̄_t\n",
    "            x_t = x_0_hat*x0_coeff + x_t*xt_coeff + self.σ[t]*z\n",
    "            preds.append(x_t.cpu())\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8170516",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 4e-3\n",
    "epochs = 5\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "ddpm_cb = DDPMCB(n_steps=1000, beta_min=0.0001, beta_max=0.02)\n",
    "cbs = [ddpm_cb, DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchschedCB(sched)]\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3658d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d77b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
